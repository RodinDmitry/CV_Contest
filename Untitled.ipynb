{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import senet50_128 as senet\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook  import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine \n",
    "import modelUtils\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Senet50_128(\n",
       "  (conv1_7x7_s2): Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (conv1_7x7_s2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1_relu_7x7_s2): ReLU(inplace=True)\n",
       "  (pool1_3x3_s2): MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
       "  (conv2_1_1x1_reduce): Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_1_1x1_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_1_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv2_1_3x3): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2_1_3x3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_1_3x3_relu): ReLU(inplace=True)\n",
       "  (conv2_1_1x1_increase): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_1_1x1_increase_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_1_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv2_1_1x1_down): Conv2d(256, 16, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_1_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv2_1_1x1_up): Conv2d(16, 256, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_1_prob): Sigmoid()\n",
       "  (conv2_1_1x1_proj): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_1_1x1_proj_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_1_relu): ReLU(inplace=True)\n",
       "  (conv2_2_1x1_reduce): Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_2_1x1_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_2_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv2_2_3x3): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2_2_3x3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_2_3x3_relu): ReLU(inplace=True)\n",
       "  (conv2_2_1x1_increase): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_2_1x1_increase_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_2_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv2_2_1x1_down): Conv2d(256, 16, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_2_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv2_2_1x1_up): Conv2d(16, 256, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_2_prob): Sigmoid()\n",
       "  (conv2_2_relu): ReLU(inplace=True)\n",
       "  (conv2_3_1x1_reduce): Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_3_1x1_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_3_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv2_3_3x3): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2_3_3x3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_3_3x3_relu): ReLU(inplace=True)\n",
       "  (conv2_3_1x1_increase): Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv2_3_1x1_increase_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_3_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv2_3_1x1_down): Conv2d(256, 16, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_3_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv2_3_1x1_up): Conv2d(16, 256, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv2_3_prob): Sigmoid()\n",
       "  (conv2_3_relu): ReLU(inplace=True)\n",
       "  (conv3_1_1x1_reduce): Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv3_1_1x1_reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_1_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv3_1_3x3): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3_1_3x3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_1_3x3_relu): ReLU(inplace=True)\n",
       "  (conv3_1_1x1_increase): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_1_1x1_increase_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_1_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv3_1_1x1_down): Conv2d(512, 32, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_1_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv3_1_1x1_up): Conv2d(32, 512, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_1_prob): Sigmoid()\n",
       "  (conv3_1_1x1_proj): Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv3_1_1x1_proj_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_1_relu): ReLU(inplace=True)\n",
       "  (conv3_2_1x1_reduce): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_2_1x1_reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_2_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv3_2_3x3): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3_2_3x3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_2_3x3_relu): ReLU(inplace=True)\n",
       "  (conv3_2_1x1_increase): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_2_1x1_increase_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_2_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv3_2_1x1_down): Conv2d(512, 32, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_2_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv3_2_1x1_up): Conv2d(32, 512, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_2_prob): Sigmoid()\n",
       "  (conv3_2_relu): ReLU(inplace=True)\n",
       "  (conv3_3_1x1_reduce): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_3_1x1_reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_3_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv3_3_3x3): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3_3_3x3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_3_3x3_relu): ReLU(inplace=True)\n",
       "  (conv3_3_1x1_increase): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_3_1x1_increase_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_3_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv3_3_1x1_down): Conv2d(512, 32, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_3_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv3_3_1x1_up): Conv2d(32, 512, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_3_prob): Sigmoid()\n",
       "  (conv3_3_relu): ReLU(inplace=True)\n",
       "  (conv3_4_1x1_reduce): Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_4_1x1_reduce_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_4_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv3_4_3x3): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3_4_3x3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_4_3x3_relu): ReLU(inplace=True)\n",
       "  (conv3_4_1x1_increase): Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv3_4_1x1_increase_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_4_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv3_4_1x1_down): Conv2d(512, 32, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_4_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv3_4_1x1_up): Conv2d(32, 512, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv3_4_prob): Sigmoid()\n",
       "  (conv3_4_relu): ReLU(inplace=True)\n",
       "  (conv4_1_1x1_reduce): Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv4_1_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_1_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_1_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_1_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_1_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_1_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_1_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_1_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_1_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_1_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_1_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_1_prob): Sigmoid()\n",
       "  (conv4_1_1x1_proj): Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv4_1_1x1_proj_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_1_relu): ReLU(inplace=True)\n",
       "  (conv4_2_1x1_reduce): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_2_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_2_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_2_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_2_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_2_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_2_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_2_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_2_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_2_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_2_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_2_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_2_prob): Sigmoid()\n",
       "  (conv4_2_relu): ReLU(inplace=True)\n",
       "  (conv4_3_1x1_reduce): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_3_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_3_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_3_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_3_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_3_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_3_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_3_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_3_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_3_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_3_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_3_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_3_prob): Sigmoid()\n",
       "  (conv4_3_relu): ReLU(inplace=True)\n",
       "  (conv4_4_1x1_reduce): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_4_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_4_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_4_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_4_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_4_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_4_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_4_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_4_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_4_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_4_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_4_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_4_prob): Sigmoid()\n",
       "  (conv4_4_relu): ReLU(inplace=True)\n",
       "  (conv4_5_1x1_reduce): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_5_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_5_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_5_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_5_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_5_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_5_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_5_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_5_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_5_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_5_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_5_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_5_prob): Sigmoid()\n",
       "  (conv4_5_relu): ReLU(inplace=True)\n",
       "  (conv4_6_1x1_reduce): Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_6_1x1_reduce_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_6_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv4_6_3x3): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4_6_3x3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_6_3x3_relu): ReLU(inplace=True)\n",
       "  (conv4_6_1x1_increase): Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv4_6_1x1_increase_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4_6_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv4_6_1x1_down): Conv2d(1024, 64, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_6_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv4_6_1x1_up): Conv2d(64, 1024, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv4_6_prob): Sigmoid()\n",
       "  (conv4_6_relu): ReLU(inplace=True)\n",
       "  (conv5_1_1x1_reduce): Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv5_1_1x1_reduce_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_1_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv5_1_3x3): Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv5_1_3x3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_1_3x3_relu): ReLU(inplace=True)\n",
       "  (conv5_1_1x1_increase): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv5_1_1x1_increase_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_1_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv5_1_1x1_down): Conv2d(2048, 128, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_1_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv5_1_1x1_up): Conv2d(128, 2048, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_1_prob): Sigmoid()\n",
       "  (conv5_1_1x1_proj): Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
       "  (conv5_1_1x1_proj_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_1_relu): ReLU(inplace=True)\n",
       "  (conv5_2_1x1_reduce): Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv5_2_1x1_reduce_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_2_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv5_2_3x3): Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv5_2_3x3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_2_3x3_relu): ReLU(inplace=True)\n",
       "  (conv5_2_1x1_increase): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv5_2_1x1_increase_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_2_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv5_2_1x1_down): Conv2d(2048, 128, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_2_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv5_2_1x1_up): Conv2d(128, 2048, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_2_prob): Sigmoid()\n",
       "  (conv5_2_relu): ReLU(inplace=True)\n",
       "  (conv5_3_1x1_reduce): Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv5_3_1x1_reduce_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_3_1x1_reduce_relu): ReLU(inplace=True)\n",
       "  (conv5_3_3x3): Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv5_3_3x3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_3_3x3_relu): ReLU(inplace=True)\n",
       "  (conv5_3_1x1_increase): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       "  (conv5_3_1x1_increase_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5_3_global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv5_3_1x1_down): Conv2d(2048, 128, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_3_1x1_down_relu): ReLU(inplace=True)\n",
       "  (conv5_3_1x1_up): Conv2d(128, 2048, kernel_size=[1, 1], stride=(1, 1))\n",
       "  (conv5_3_prob): Sigmoid()\n",
       "  (conv5_3_relu): ReLU(inplace=True)\n",
       "  (pool5_7x7_s1): AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
       "  (feat_extract): Conv2d(2048, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = senet.senet50_128('senet50_128.pth')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=''):\n",
    "    mean = (131.0912, 103.8827, 91.4953)\n",
    "    img = Image.open(path)\n",
    "    img = img.convert('RGB')\n",
    "    x = np.array(img)\n",
    "    x = x - mean\n",
    "    return x\n",
    "\n",
    "def process_image(image_name):\n",
    "    test_image = load_data(image_name)\n",
    "    test_image = np.expand_dims(test_image, 0)\n",
    "    tensor = torch.Tensor(test_image.transpose(0, 3, 1, 2))\n",
    "    tensor = tensor.to(device)\n",
    "    f = model(tensor)[1].detach().cpu().numpy()[:, :, 0, 0].flatten()\n",
    "    f = f / np.sqrt(np.sum(f ** 2, -1, keepdims=True))\n",
    "    return f\n",
    "\n",
    "def load_all(folder):\n",
    "    result = []\n",
    "    image_names = os.listdir(folder)\n",
    "    for image_name in tqdm(image_names):\n",
    "        image_path = os.path.join(folder, image_name)\n",
    "        image_data = process_image(image_path)\n",
    "        result.append(image_data)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = os.listdir()\n",
    "random = np.random.randint(len(train_folders), size = 500)\n",
    "subfolders = [train_folders[idx] for idx in random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('C:/Datasets/train_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Datasets/train_data.npy', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = modelUtils.DataLoader(train_data, 1024)\n",
    "data, labels = loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, steps):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for step in range(steps):\n",
    "        batch, labels = train_loader.next()\n",
    "        batch = torch.Tensor(batch).to(device)\n",
    "        labels = torch.Tensor(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = modelUtils.Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2 = torch.load('D:/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "m_2.to(device)\n",
    "optimizer = torch.optim.SGD(m_2.parameters(), lr=0.001)\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "epochs = 5000\n",
    "with tqdm(range(epochs)) as t:\n",
    "    for epoch in t:\n",
    "        loss_average = train(m_2, device, loader, optimizer, 100)\n",
    "        t.set_description('Loss = {}'.format(loss_average))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_predict, 'D:/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = load_all('C:/Datasets/test_data_processed/test_data/')\n",
    "test_data = np.load('C:/Datasets/test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:/Datasets/test_data.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv(r'C:/Datasets/class_mapping.csv')\n",
    "names = np.array(mapping['Id'])\n",
    "test_labels = np.array(mapping['Category'])\n",
    "test_folder = r'C:/Datasets/test_data_processed/test_data/'\n",
    "test_images = os.listdir(test_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Datasets/names.npy', test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(name):\n",
    "    name = int(name.split('.')[0])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = []\n",
    "labeled_idx = []\n",
    "for name in names:\n",
    "    idx = get_idx(name)\n",
    "    labeled_data.append(test_data[idx])\n",
    "labeled_data = np.array(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(np.dot(test_data, labeled_data.T), axis=1)\n",
    "labels = test_labels[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KMeans(n_clusters=500, init = labeled_data, verbose = 1, n_jobs=-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not labeled_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cosine = KMeansClusterer(num_means=500, \n",
    "                                    distance=nltk.cluster.util.euclidean_distance,\n",
    "                                    conv_test=1e-5, \n",
    "                                    initial_means=labeled_data.tolist(), svd_dimensions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (169396, 169396) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7eb46baf75b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_cosine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\nltk\\cluster\\util.py\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(self, vectors, assign_clusters, trace)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# use SVD to reduce the dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_svd_dimensions\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_svd_dimensions\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             S = d[: self._svd_dimensions] * numpy.identity(\n\u001b[0;32m     54\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_svd_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1636\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1637\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (169396, 169396) and data type float64"
     ]
    }
   ],
   "source": [
    "clusters = classifier_cosine.cluster(test_data, True, trace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodin\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True,\n",
       "       init=array([[ 0.13564809,  0.06718813, -0.28028584, ..., -0.02790043,\n",
       "         0.06933365, -0.04167731],\n",
       "       [ 0.18854925, -0.00574087, -0.02355394, ...,  0.01835684,\n",
       "         0.07411209, -0.03638139],\n",
       "       [ 0.04883622,  0.02669334, -0.00921401, ...,  0.03166592,\n",
       "         0.09742534,  0.10344726],\n",
       "       ...,\n",
       "       [ 0.11027922,  0.0466456 ,  0.00521537, ..., -0.0424542 ,\n",
       "        -0.0759325 , -0.13310029],\n",
       "       [ 0.03704501,  0.16111533,  0.02024144, ...,  0.03167344,\n",
       "         0.00588075,  0.2220037 ],\n",
       "       [ 0.15249005, -0.08707568,  0.06393636, ...,  0.06118096,\n",
       "         0.14565364,  0.08753508]], dtype=float32),\n",
       "       max_iter=300, n_clusters=500, n_init=10, n_jobs=-1,\n",
       "       precompute_distances='auto', random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_labels[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef62b8ebbea49208fcba4ec674f48f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=169396.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Senet50_128' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c0dec6bc0dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvec_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtiled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    589\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 591\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Senet50_128' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "labels = np.zeros(len(test_data))\n",
    "values = np.zeros(len(test_data))\n",
    "for vec_id, vector in enumerate(tqdm(test_data)): \n",
    "    tiled = np.tile(vector, len(labeled_data)).reshape(-1, len(vector))\n",
    "    prediction = np.argmax(model.predict([labeled_data, tiled]))\n",
    "    labels[vec_id] = test_labels[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41d2a9e9bc44ba981c5e301266e40f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict = {'Id':[], 'Random':[]}\n",
    "for idx, image in tqdm(enumerate(test_data)):\n",
    "    image_name = test_images[idx]\n",
    "    result_dict['Id'].append(image_name)\n",
    "    result_dict['Random'].append(int(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame(result_dict)\n",
    "result_frame.to_csv('D:/result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.linalg.norm(label_data, axis=1)\n",
    "n2 = np.linalg.norm(test_data, axis=1)\n",
    "product =  np.dot(label_data, test_data.T) \n",
    "argmax = np.argmax(product, axis = 0)\n",
    "print(argmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'Id':[], 'Random':[]}\n",
    "for idx, sim_id in enumerate(argmax):\n",
    "    image_name = test_images[idx]\n",
    "    result_dict['Id'].append(image_name)\n",
    "    result_dict['Random'].append(labels[sim_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
